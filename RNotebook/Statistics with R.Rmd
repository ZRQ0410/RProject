---
title: "Statistics with R"
output: 
  html_notebook:
    toc_float: true
    toc: true
---

------------------------------------------------------------------------

# 1. Descriptive Statistics {#1}

### Calculation

-   `exp(x)` means e\^x in maths

-   `log(x)` means log(x, exp(1))

```{r}
3^2
3**2
sqrt(16)
log(9, 3)
```

### Defining values, vectors

```{r}
x <- 2
y = 2
z <- c(1,3,5) # vector
z[1:2] # 1 2
```

```{r}
5 + c(1,2,3) # 6 7 8
5 * c(1,2,3) # 5 10 15
```

```{r}
data.frame(mycol1=x, mycol2=z)
```

### Mean, range, median, summary, table

`mean()` `range()` `median()`

-   na.rm=T

    not available, remove, true

`summary()` `table()`

-   summary and table do not need na.rm=T

-   use table() to find the mode

```{r}
mean(kc$weight, na.rm=T)
table(kc$weight)
```

### Variance, standard deviation

`var()` `sd()`

-   na.rm=T

### Histogram, plot

`hist(..., breaks=seq(start,end,step), col=, main=, xlab=, ylab=, main=, freq=)`

-   breaks: breaks=seq(0,65,1), from 0 to 65, increment by 1

-   col: color

-   xlab, ylab: labels of x and y axes

-   main: title of the gragh

-   freq=T: count

    freq=F: percentage

```{r}
hist(kc$weight, breaks=seq(0,65,1), col="lightblue", main="Weight of !Kung", xlab="weight (kg)", ylab="number of people")
```

`plot(y ~ x, col=, xlab=, ylab=, main=, cex.axis=, pch=)` or `plot(x, y)`

-   cex.axis: font size

-   pch: choose the symbol of the points ï¼ˆé€‰æ‹©ç‚¹çš„æ ·å¼ï¼‰

```{r}
plot(kc$weight ~ KungCensus$age, cex.axis=1.2, col="grey65", xlab="age (years)", ylab="weight")
```

\

------------------------------------------------------------------------

# 2. Distribution {#2}

### Normal distribution

Gaussian distribution, bell-curve

-   mean value is the most likely value (peak)

-   probability of a value decreases with distance to mean

-   sum of all probabilities is 100%

### Standardisation

**z-score**

$$
Z = \frac{x - Âµ}{ðž¼}\qquad
$$

-   $Âµ$: mean

-   $ðž¼$: standard deviation

We can **standardise** variables so that everything we measure has **mean=0** and **sd=1**.

### Probability

**`pnorm(test value, mean, sd)`** = `pnorm(z-score, 0, 1)`

-   calculate **cumulative** probability:

    all probabilities that \<= test value

-   lower interval: pnorm()

    upper interval: 1-pnorm()

### Confidence interval

When $âº$ = 0.05, confidence interval = 95%

-   z=-1.96 and z=1.96

-   from 2.5% \~ 97.5%

------------------------------------------------------------------------

# 3. T-test {#3}

### **Assumption:** **Confidence intervals and all t-tests assume a [normal distribution]{.underline}!**

### Null hypothesis (H~0~):

-   difference is not large enough

-   no significant difference

-   difference is just a random outcome form sampling

### **One-sample t-test**

Does a group differ from a **fixed value**?

#### test statistic: t-statistic, t-score

$$
t = \frac{x - Âµ}{sem}\qquad
$$

-   $Âµ$: mean

-   $sem$: standard error of the mean, $sem = ðž¼/\sqrt{n-1}$

`t.test(data, mu=, conf.level=)`

-   mu: test value

-   conf.level: change confidence interval, default is 0.95

```{r}
# weight is defferent from 40kg?
t.test(kaf$weight, mu=40)
# we are 95% sure that mean weight is between 41.33 and 42.82

t.test(kaf$weight, mu=40, conf.level=0.99)
```

### **Two-sample t-test**

Do groups differ?

```{r}
t.test(kc$weight ~ kc$sex)
# 95% sure that weight difference bwtween sex is between 3.66 to 8.40

t.test(kc$weight ~ kc$sex, conf.level=0.99)
```

### **Paired t-test**

Two sets of measurements of the **same individuals**. (**before** and **after**)

Two measurements are not independent.

`t.test(data1, data2, paired=T)`

```{r}
library(ISwR) # need intake dataset in ISwR

t.test(intake$post, intake$pre, paired=T)
t.test(intake$post, intake$pre, paired=T, conf.level=0.99)
```

### **Result (When** $âº$**=5%)**

-   If P \> 0.05, null hypothesis is accepted: no difference

-   If **P \< 0.05**, **alternative hypothesis** is accepted: **significant difference**

------------------------------------------------------------------------

# 4. Non-parametric tests {#4}

### When to run a non-parametric test:

-   sample is large, but **not normal**

-   if one group is **too small** (less than 20 data) ï¼ˆåªè¦å…¶ä¸­ä¸€ä¸ªç»„æ ·æœ¬å°ï¼Œå°±ç”¨Non-paraï¼‰

### **Check for normality**

#### 1. Look for bell-shaped histogram (visual check) {#visual_check}

```{r}
hist(intake$post, breaks=seq(3500, 8000,100))
```

#### 2. **Shapiro-Wilk test** {#shapiro}

`shapiro.test()`

-   if P \> 0.05, variable is normal

-   if P \< 0.05, variable is not normal

-   if sample size is:

    -   **too large: too sensitive!**

        -   [So if we have a bell-shaped large sample but fail the test, still treat it as normal!]{.underline}

    -   too small: not sensitive! do not use

```{r}
shapiro.test(kaf$height) # p=0.6761, normal
```

### **Non-parametric test (Wilcoxon test)**

### One sample

`wilcox.test(data, mu=6500, conf.int=)`

-   conf.int=T: display confidence intervals in the result

```{r}
wilcox.test(intake$post, mu=6500, conf.int=T)
# p = 0.01855, significant difference
```

### Two-sample

```{r}
wilcox.test(kb$weight ~ kb$sex, conf.int=T)
# p = 0.6612, no significant difference
```

### Paired samples

`wilcox.test(data1, data2, paired=T, conf.int=T)`

```{r}
wilcox.test(intake$pre, intake$post, paired=T, conf.int=T)
# p = 0.00384, difference
```

Ps. ignore the warning

------------------------------------------------------------------------

# 5. Proportion tests (Chi-square tests) {#5}

### One-sample proportion

Only **one independent proportion** here: man \$ woman, children & adults.

Proportions must add up to 100%.

`prop.test(x=positives, n=total, p=test proportion)`

```{r}
prop.test(134, 254, 0.5)
# p = 0.4147, no significant difference from 0.5
```

### Two independent proportions

`prop.test(c(pos1, pos2â€¦), c(total1, total2â€¦))`

```{r}
prop.test(c(120,114), c(254,216))
# p = 0.2699, no significant difference
```

### More than two independent proportions

`prop.test(c(pos1, pos2, pos3, pos4), c(total1, t2, t3, t4))`

`chisq.test(matrix(c(pos1, pos2, pos3, pos4, neg1, neg2, neg3, neg4), m))`

-   m: number of the groups, here m=4

`fisher.test(matrix(c(pos1, pos2â€¦, neg1, neg2â€¦), m))`

------------------------------------------------------------------------

# 6. Analysis of variance (ANOVA) {#6}

### **When to use ANOVA:**

-   compare **three or more groups**

-   **all groups are normally distributed**

-   åªè¦æœ‰ä¸€ç»„ä¸æ˜¯normalï¼Œå°±åªèƒ½ç”¨non-para

### 1. Check normality:

[*visual inspection*](#visual_check), [*Shapiro-Wilks test*](#shapiro)

### 2. If samples are [large]{.underline}:

-   **check** for equality of **variance**: [Bartlett's test](#bartlett_anova)

    -   if variances are similar:

        -   [`anova(lm)`](#bartlett_anova), [**grouping variable must be a factor!**]{.underline}

        -   if group means differ, **pairwise t-test** with Holm correction

    -   if variables differ:

        -   [`oneway()`](#bartlett_anova)

        -   if group means differ, **pairwise t-test not assuming equal variances**, Holm correction

### 3. If samples are [small]{.underline} or [not normal]{.underline}:

-   [Kruskal-Wallis test](#kruskal) (do not run Bartlett's test / ANOVA!)

    -   if groups means differ, **pairwise Wilcoxon test**, Holm correction

### **ANOVA**

x~i~ = general mean + (between-group difference) + (within-group difference)

#### **Squared differences**

S = total sample squared differences

S~b~ = sum of all squared between-group differences

S~w~ = sum of all squared within-group differences

S = S~b~ + S~w~

#### **Variances**

M~b~ = S~b~ / k-1 = between-group variance

M~w~ = S~w~ / N-k = within-group variance

-   k: group numbers

-   N: total data

#### **F-ratio**

$$
F = \frac{M_b}{M_w}\qquad
$$

-   F \~ 1, groups do not differ

-   F \> 1, there is a group effect

### Bartlett's test & ANOVA {#bartlett_anova}

#### **Bartlett's test**

Check equality of variances across groups.

`bartlett.test(variable ~ grouping variable)`

#### **ANOVA**

-   if variances are similar across groups = **not significant**:

    -   use **`anova(lm)`**

        -   grouping variable should always be a **factor** here!!! (otherwise lm would run linear regression)

        -   `file$variable <- as.factor(file$variable)`

    -   pairwise t-test

        `pairwise.t.test(variable ~ grouping variable)`

-   if variances differ = **significant different**:

    use **`oneway()`**

    -   pairwise t-test, not assuming equal variances

        -   `pairwise.t.test(variable, grouping variable, pool.sd=F)`

```{r}
# anova(lm) test:

bartlett.test(SBR2$head ~ SBR2$year)
# p = 0.2929, not significant, use anova(lm)

anova(lm(SBR2$head ~ SBR2$year))
# F-value = 17.141, p = 3.987e-11, significant different

# see which group is different
pairwise.t.test(SBR2$head, SBR2$year)
# except 2004~2005, all different
```

Analysis of Variance Table

|            |   Df   | Sum Sq | Mean Sq | F value |  Pr(\>F)  |
|:----------:|:------:|:------:|:-------:|:-------:|:---------:|
| SBR2\$year |   3    |  165   | 55.081  | 17.141  | 3.987e-11 |
| Residuals  | 190857 | 613297 |  3.213  |         |           |

Row1:

-   degree of freedoms, group number - 1 = 4 - 1 = 3

-   S~b~ sum of all squared [between-group]{.underline} differences

-   M~b~ [between-group]{.underline} variance

-   F-ratio

-   P-value

Row2:

-   observations without NAs

-   S~w~ sum of all squared [within-group]{.underline} differences

-   M~w~ [within-group]{.underline} variance

```{r}
# anova oneway test:

bartlett.test(SBR3$head ~ SBR3$delivery)
# p < 2.2e-16, variances are different

oneway.test(SBR3$head ~ SBR3$delivery) # p-value < 2.2e-16

# pairwise t-test
pairwise.t.test(SBR3$head, SBR3$delivery, pool.sd=F)
```

### **Non-parametric alternative** {#kruskal}

If sample size is small / not normal: Do not run Bartlett's test / ANOVA!

Bartlett's test will return a high p-value.

#### Kruskal-Wallis test:

`kruskal.test(variable ~ grouping variable)`

#### pairwise Wilconxon test:

`pairwise.wilcox.test(variable, grouping variable)`

```{r}
kruskal.test(brain$PrebyT ~ brain$group)

pairwise.wilcox.test(brain$PrebyT, brain$group)
```

------------------------------------------------------------------------

# 7. Power and sample size {#7}

### Type 1 error

-   incorrectly reject a 'true' null hypothesis

-   false positive

-   **difference does not really exist**

-   probability of type 1 error is the significance level

### Type 2 error

-   accept a 'wrong' null hypothesis

-   false negative

-   **difference is there, but you cannot see it**

-   test is not powerful enough to identify true difference (short-sighted)

### Statistical power (test resolution)

-   power to identify a true difference

-   **power to avoid type 2 error**

-   if power of test = 0.9, chance of type 2 error is 0.1

-   should be at least 80%

### Power of one-sample t-test, two-sample t-test, paired t-test, two-proportion test, ANOVA test

```{r}
# one sample t-test
power.t.test(n=20, sd=7, delta=5, type="one.sample")
power.t.test(power=0.9, sd=7, delta=5, type="one.sample")
# for one-tailed: alt="one.sided"

# two sample (default)
# to calculate, use smaller sample size, larger sd!
power.t.test(power=0.8, delta=7, sd=5)

# paired
power.t.test(delta=2, sd=5, power=0.80, type="paired")
# n for pairs, total data: 2n

# proportion
power.prop.test(power=0.9, p1=0.1, p2=0.25)

# anova
power.anova.test(groups=4, between.var=4, within.var=40, power=0.9)

```

-   ! å¾—åˆ°çš„æ ·æœ¬æ•°å‘ä¸Šå–æ•´ï¼ï¼ï¼ˆ13.2 -\> 14ï¼‰

-   the power test assume sample is normal

-   if power calculation suggest very small sample size:

    -   use non-parametric tests

    -   or use a larger sample

------------------------------------------------------------------------

# 8. Correlation & Linear regression {#8}

### **Significant test of correlation**

H~0~: r = 0, no correlation

**correlation** is the **standardised covariance** of x and y

-   r = 1: absolute association

-   r = -1: absolute but inverse association

-   r = 0: no association

### 1. If normal: Pearson correlation

`cor.test(variable1, variable2)`

```{r}
cor.test(SBR$size, SBR$head)
# t = 319.7, p < 2.2e-16, cor = 0.5945857, relatively strong association
```

### 2. If [small]{.underline} size or [not normal]{.underline}: Spearman's correlation

`cor.test(variable1, variable2, method="spearman")`

```{r}
cor.test(Brains2$PreWhite, Brains2$PreGray, method="spearman")
# p = 0.0001219, rho = 0.7936017, strong association
```

### **Linear regression**

H~0~: b = 0 (horizontal line), x has no effect on y

```{r}
brainreg <- lm(Brains$BrWhite ~ Brains$BrGray) # lm: linear model
summary(brainreg)
# a not different from 0, b significant different from 0
# a = -1.44510, b = 1.21928
# Goodness-of-fit: Multiple R-squared: 0.9673

# confidence interval of parameters
confint(brainreg)

# visualising
plot(Brains$BrWhite ~ Brains$BrGray)

# adding regression line
abline(brainreg)
```

Goodness-of-fit:

-   Multiple R-squared in linear regression

-   shows how much of variance in data is explained by the model

### Create a linear model:

1.  plot variables

2.  test significance of regression slope

3.  if slope significant: use model y = a + bx, interpret meaning of intercept and slope

4.  report confidence interval and goodness-of-fit

------------------------------------------------------------------------

# 9. Principal component analysis (PCA) {#9}

-   PCA is [**useful when**]{.underline} original variables are [**correlated**]{.underline}!

-   each PC is

    -   mutually independent, orthogonal

    -   ordered by importance

-   Y score of PC1 is:

    -   Y: score of a specific case

    -   b: loading of variable

    -   X: original measurements of the specific case

$$
Y = b_1X_1 + b_2X_2...+ b_nX_n
$$

### Data processing:

-   include only the needed variables

    -   include: `newfile <- subset[oldfile, select=c(variable1â€¦)]`

    -   or exclude: `newfile <- subset[oldfile, select=-c(variable1â€¦)]`

-   remove all NAs

    -   eliminate all NAs: `newfile <- oldfile[complete.cases(oldfile),]`

    -   eliminate NAs for one variable only: `newfile <- oldfile[complete.cases(oldfile$variable)]`

### PCA

`prcomp(data, scale.=T, retx=T)`

-   scale.=T: scales variance to 1, use a **correlation matrix** (standardised, mean=0 and sd=1)

    if =F, use a covariance matrix

-   retx=T: Y score is returned

-   When calculating score y of a specific case, use **z-score** if scale.=T!

    -   Or, use `mymodel$x` to see all y scores.

```{r}
# include only the measurements
lifehistory2 <- subset(lifehistory, select=-c(group,species))
#  exclude lines with NAs
lifehistory <- lifehistory[complete.cases(lifehistory),]

# run PCA
pca1 <- prcomp(lifehistory2, scale.=T, retx=T)
pca1

# see y scores for all cases
# pca1$x
```

### PC retention criteria

1.  $\lambda > 1$

```{r}
# show eigenvalues (variance)
pca1$sdev^2 
# 3.48705601 > 1, retain PC1
```

2.  scree plots

```{r}
screeplot(pca1, main="PCs", pch=16)
screeplot(pca1, main="PCs", type="line", pch=16)
title(xlab="PC number")
screeplot(pca1, main="PCs", pch=16, col=c("orange", "lightblue"))
# keep only PC1
```

3.  fraction of variance

```{r}
summary(pca1)
# proportion of PC1: 0.8718
# keep PC1 (or keep PC1 and PC2)
```

4.  interpretability: use the rules together, explain the loadings (positive and negative)

### (Grouping cases)

*see the slides*

------------------------------------------------------------------------

# 10. Logistic regression {#10}

-   we use generalised linear model

-   predict binary responses

-   form of the regression:

$$probability\ of\ binary\ outcome = a + b_1X_1 + b_2x_2... + B_nX_n = a + \sum{b_iX_i}$$

-   a = intercept

-   b~i~ = regression coefficient

-   X~i~ = independent variables

#### **odds**

$$
odds\ of\ event = \frac{prob\ of\ event\ occuring}{prob\ of\ event\ not\ occuring} = \frac{p}{1-p}
$$

#### **odds ratio**

$$
odds\ ratio = \frac{odds\ of\ event1}{odds\ of\ event2}
$$

#### **logistic function**

$$
y = \frac{1}{1 + e^{-f}} = p
$$

-   here p is probability, range of the function is $(0, 1)$

#### **logit p**

$$
logit\ p = log(\frac{p}{1-p}) = log(odds) = a + \sum b_iX_i
$$

#### **probability p of event**

$$
p = \frac{1}{1 + e^{-logit}} = \frac{1}{1 + e^{-(a+\sum bX)}}
$$

### One categorical variable (2 levels, binary)

variable X = 0 / 1: $logit\ p = log(odds) = a + bX$

-   X = 0, baseline

    -   $log(baseline\ odds) = a + b.0 = a$

    -   $odds = e^a$

    -   $p = \frac{1}{1+e^{-a}} = \frac{e^a}{e^a+1}$

-   X = 1, exposure

    -   $log(exposure\ odds) = a + b.1 = a + b$

    -   $odds = e^{a+b} = e^ae^b$

    -   $p = \frac{1}{1+e^{-(a+b)}} = \frac{e^{a+b}}{e^{a+b}+1}$

-   $odds\ ratio = \frac{exposure\ odds}{baseline\ odds} = e^b$

`model.name <- glm(y ~ x, data=, family=binomial)`

`summary(model.name)`

```{r}
model.hyper <- glm(hypnonhyp ~ obesity, data=hypertension, family=binomial)
summary(model.hyper) 
# p = 0.00718, significant difference
# intercept a = -1.6762, a != 0, e^a (baseline odds) != 1, prob != 0.5
# coefficient b = 0.7600, e^b = odds ratio > 1
```

-   **Do not forget 'binomial'!**

-   Goodness-of-fit: AIC, the **lower** the AIC, the **better** the model

### Continuous variable

-   predict logits:

    `predict(model.name)`

-   predict probability:

    `probs <- predict(model.name, type="response")`

    `plot(probs ~ predictor, data=)`

-   predict probabilities at a given point or X value:

    `predict(model.name, data.frame(X=value), type="response")`

-   calculate Median:

    $p = 0.5,\ odds = 1,\ logit = log(1) = 0,\ 0 = a + bX$

```{r}
model.menar <- glm(menarche ~ age, binomial, data=menar)
summary(model.menar) # b = 1.5173, significant
# odds ratio is change in odds of event (menarche) when age increase by one (unit)

# logits
predict(model.menar)

# probabilities
probs <- predict(model.menar, type="response")
probs

# probability plot
plot(probs ~ age, data=menar, ylab="menarche", pch=16)

# predict probability at a given point
predict(model.menar, data.frame(age=20), type= "response")
```

### Categorical variable with \>2 levels

-   first level is baseline

-   **all other** are **compared to the baseline**

-   **levels**, not continuous, should be **factors**! (`as.factor()`)

```{r}
model.infant <- glm(healthy ~ month, binomial, data=infant)
summary(model.infant)
coef(model.infant)
exp(coef(model.infant))
levels(infant$month)

# set April as baseline
infant$month <- relevel(infant$month, ref = 4)
model.infant <- glm(healthy ~ month,binomial, data=infant)
summary(model.infant)
```

### Interactions

-   **additive effect** of A and B:

    A, B are independent (multiply effects)

-   **interactive / multiplicative effect** of A and B:

    A, B are dependent

    -   positive interaction: effects are stronger when combined

    -   negative interaction: effects are cancelled or reduced when combined

-   effect of variables X~1~, X~2~, X~3~: Y \~ X~1~ \* X~2~ \* X~3~

    this generates: X~1~ + X~2~ + X~3~ + X~1~:X~2~ + X~1~:X~3~ + X~2~:X~3~ + X~1~:X~2~:X~3~

    (**interactions** are represented by "**:**")

`model.name <- glm(y ~ x1*x2*x3, binomial, data=)`

`summary(model.name)`

```{r}
model.chd <- glm(chd ~ age*cat*chl, binomial, data=evans)
summary(model.chd)
```

-   b for age:cat interaction is b=-0.53, this means the effects of cat and age are reduced

### Model optimisation

-   eliminate non significant terms

-   Hierarchy Principle:

    higher order interactions are tested first

    -   if they are significant, all lower level interactions and single terms must be kept even if they are not significant

    -   if interaction X~1~:X~2~:X~3~ is significant, final model must also include:

        -   X~1~ , X~2~ , X~3~

        -   X~1~:X~2~, X~1~:X~3~, X~2~:X~3~

    -   we need odds in baseline if we want to estimate odds in exposure group

-   when doing any calculations, use values **in the final model**! (values may change after optimisation)

`summary(step(model.name))`

```{r}
model.menar2 <- glm(menarche ~ age*igf1, binomial, data=menar)
summary(model.menar2)
summary(step(model.menar2))
```

-   after optimisation, logit of y: (if all variables X~1~, X~2~ and interactions are kept)

    Y = a + b~1~\*(X~1~) + b~2~\*(X~2~) + b~3~\*(X~1~\*X~2~)

-   Goodness-of-fit: Models with **lowest AIC** are selected.

### Some calculations

-   **exp(a) = baseline odds**

-   **exp(b) = odds ratio**

-   **exp(a+b) = exposure odds**

-   **p = odds/(odds + 1)**

------------------------------------------------------------------------

# Appendix

Get subset of the file:

`newfile <- subset(file, condition)`

```{r}
kungadultfemales <- subset(kc, age > 18 & sex == 'woman')
```

Deal with several groups at the same time:

```{r}
tapply(SBR2$head, SBR2$year, var, na.rm=T)
```

[Shapiro test](#shapiro) will raise an error if the sample size \> 5000:

```{r}
tapply(SBR2$size, SBR2$delivery, function(x) shapiro.test(sample(x,4999)))
```
